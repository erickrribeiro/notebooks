{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como formatar as respostas do GPT utilizando a biblioteca LangChain\n",
    "\n",
    "> Nota: também disponível no [Medium.com](https://medium.com/@erickrribeiro/uso-de-resultados-estruturados-e-formatados-com-gpt-e-langchain-f7d57d1f28c9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-requisitos\n",
    "\n",
    "Para executar os códigos-fonte apresentados nesse notebook é necessário instalar as seguintes bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai==0.27.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI: 0.27.8\n",
      "LangChain: 0.0.304\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import langchain\n",
    "\n",
    "print(\"OpenAI:\", openai.__version__)\n",
    "print(\"LangChain:\", langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chave de API da OpenAI\n",
    "\n",
    "O LLM da OpenAI é um serviço pago. Então, para utilizá-lo é necessário criar uma conta na OpenAI e gerar uma chave de API. Para mais detalhes, consulte o [link](https://openai.com/blog/openai-api). Após obter a chave, crie uma variável de ambiente da seguinte forma:\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=<SUBSTITUA_PELA_CHAVE>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# Também é possível carregar um arquivo .ev\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "\n",
    "O prompt é o texto de entrada que deve ser enviado para o LLM, e serve para guiar o LLM na execução de uma tarefa. Durante a construção de um prompt é possível definir uma persona, incluir exemplos, instruções e regras. O modelo usa as informações contidas no prompt para compreender o contexto e gerar respostas coerentes. O LangChain fornece várias classes e funções para facilitar a construção de prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sua tarefa é corrigir erros de ortografia.\n",
      "Corrija o texto: 'Estou certo   de que esse artigo vai me ajuudar no trnaalho'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"Sua tarefa é corrigir erros de ortografia.\\nCorrija o texto: '{text}'\"\n",
    "prompt_template = PromptTemplate(\n",
    "  input_variables=[\"text\"],\n",
    "  template=template)\n",
    "prompt = prompt_template.format(\n",
    "  text=\"Estou certo   de que esse artigo vai me ajuudar no trnaalho\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo da Resposta:  <class 'str'>\n",
      "Texto Corrigido: \n",
      "\n",
      "Estou certo de que este artigo vai me ajudar no trabalho.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"Sua tarefa é corrigir erros de ortografia.\\nCorrija o texto: '{text}'\"\n",
    "prompt_template = PromptTemplate(\n",
    "  input_variables=[\"text\"],\n",
    "  template=template)\n",
    "\n",
    "llm = OpenAI()\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "response = chain.run({\"text\": \"Estou certo   de que esse artigo vai me ajuudar no trnaalho\"})\n",
    "\n",
    "print(\"Tipo da Resposta: \", type(response))\n",
    "print(\"Texto Corrigido:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicações\n",
    "\n",
    "Note que a resposta do LLM é um texto sem erros de digitação. Agora, vamos analisar como é feito o encadeamento do Prompt com o LMM da OpenAI:\n",
    "- Primeiro, é criada uma instância do modelo de linguagem da OpenAI, com o trecho llm = OpenAI()\n",
    "- Depois, é criada uma instância da classe LLMChain. Como já foi dito, um Chain liga dois ou mais componentes. Na ocasião, a Chain é criada com o objetivo de conectar o PromptTemplate ao LLM da OpenAI.\n",
    "- Finalizando, é chamado o método run(), no trecho chain.run. Note, que o texto a ser corrigido é passado como um dicionário na chamada do método. Além disso, é utilizadas a parâmetro text, definido na classe PromptTemplate, no trecho input_variables=[\"text\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template + LLM + OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_text': 'Estou certo de que esse artigo vai me ajuudar no trnaalho',\n",
       " 'corrected_text': 'Estou certo de que esse artigo vai me ajudar no trabalho'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import (\n",
    "    ResponseSchema,\n",
    "    StructuredOutputParser,\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# inicia modelo\n",
    "llm = OpenAI()\n",
    "\n",
    "# definir o formato da resposta\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"original_text\", description=\"texto de entrada no formato bruto.\"),\n",
    "    ResponseSchema(name=\"corrected_text\", description=\"texto após correção ortográfica.\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "# Definicao do Prompt\n",
    "template = \"Sua tarefa é corrigir erros de ortografia.{format_instructions}\\nCorrija o texto: '{text}'\"\n",
    "prompt_template = PromptTemplate(\n",
    "  input_variables=[\"text\"],\n",
    "  template=template,\n",
    "  partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "response = chain.run({\"text\": \"Estou certo   de que esse artigo vai me ajuudar no trnaalho\"})\n",
    "response = output_parser.parse(response)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estou certo de que esse artigo vai me ajudar no trabalho'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"corrected_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"corrected_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Nesse artigo, foi mostrado como formatar as respostas geradas pelo modelo GPT, com uso da biblioteca LangChain. O ato de formatar as respostas pode ajudar desenvolvedores que trabalham com LLMs e buscam não apenas a saída no formato de texto livre, mas também respostas formatadas como classes, dicionários ou listas. Continue experimentando, construindo e criando, pois a jornada para uma integração mais profunda e eficaz com modelos de linguagem está apenas começando. A \"LangChain\" e tecnologias semelhantes estão aqui para capacitar sua imaginação e permitir que você transforme ideias em realidade de maneiras que nunca antes foram possíveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
